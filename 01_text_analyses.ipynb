{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text analyses of all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import logging,sys\n",
    "from getpass import getpass \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in raw data and compile the tweet hashtags and tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
    "datfiles = [f for f in files if 'April' in f]\n",
    "datfiles\n",
    "#files in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts=[]\n",
    "tag_list=[]\n",
    "for fname in datfiles:\n",
    "    print(\"Now we are processing data of\", fname)\n",
    "    with open(fname, 'r') as f:\n",
    "        my_tweets = json.load(f)\n",
    "    if my_tweets[-1] is None:\n",
    "        del my_tweets[-1]\n",
    "    for a_tweet in my_tweets:\n",
    "        if 'lang' in a_tweet:\n",
    "            if a_tweet['lang']!='en':  #filtering out all the non-english tweets\n",
    "                continue\n",
    "        if 'retweeted_status' in a_tweet: #filtering out retweets\n",
    "            continue  \n",
    "        if 'entities' in a_tweet:\n",
    "            tag_list.append(a_tweet[\"entities\"][\"hashtags\"])\n",
    "        if 'extended_tweet' in a_tweet:\n",
    "            texts.append(a_tweet[\"extended_tweet\"][\"full_text\"]) #full text of truncated tweets\n",
    "        elif 'text' in a_tweet:\n",
    "            texts.append(a_tweet[\"text\"])\n",
    "        \n",
    "    print(\"Now the texts length is\", len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in 328,223 English tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyses of the hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I am interested in knowing the occurences of all the hashtags in the tweets, and co-occurences of hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_unigrams = []\n",
    "tag_bigrams = []\n",
    "tag_trigrams = []\n",
    "\n",
    "for tags in tag_list:\n",
    "    if len(tags) is 0:\n",
    "        continue  \n",
    "    a_tweet_tags = []\n",
    "    a_tweet_bigrams=[]\n",
    "    a_tweet_trigrams=[]\n",
    "    for idx,tag in enumerate(tags):  # Enumerate returns a tuple: (index, object) for each object in the list\n",
    "            a_tweet_tags.append(tag[\"text\"].lower())\n",
    "            for tag2 in tags[(idx+1):]:  # From (idx+1) to end--all later tags\n",
    "                a_bigram = [tag[\"text\"].lower(), tag2[\"text\"].lower()]   # So they're all lowercase\n",
    "                a_bigram.sort() \n",
    "                a_tweet_bigrams.append(tuple(a_bigram))\n",
    "            for tag3 in tags[(idx+2):]:\n",
    "                a_trigram = [tag[\"text\"].lower(), tags[(idx+1)][\"text\"].lower(), tag3[\"text\"].lower()]\n",
    "                a_trigram.sort()\n",
    "                a_tweet_trigrams.append(tuple(a_trigram))\n",
    "    tag_unigrams.extend(set(a_tweet_tags)) #to make sure that repetitive hashtags in each tweet only appear once\n",
    "    tag_bigrams.extend(set(a_tweet_bigrams)) \n",
    "    tag_trigrams.extend(set(a_tweet_trigrams)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(tag_unigrams))\n",
    "print(len(tag_bigrams))\n",
    "print(len(tag_trigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tag_count = Counter(tag_unigrams)\n",
    "print(tag_count.most_common(20))\n",
    "print(\"\\n\")\n",
    "tag_bigram_count = Counter(tag_bigrams)\n",
    "print(tag_bigram_count.most_common(10))\n",
    "print(\"\\n\")\n",
    "tag_trigram_count = Counter(tag_trigrams)\n",
    "print(tag_trigram_count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to delete all the items that are or contain #nationalsiblingsday, #nationalsiblingday, #siblingsday, and #siblingday, to make a better representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exclude_list = ['nationalsiblingsday', 'nationalsiblingday', 'siblingsday', 'siblingday']\n",
    "tag_unigrams_c = [tag for tag in tag_unigrams if tag not in exclude_list]\n",
    "tag_count = Counter(tag_unigrams_c)\n",
    "print(tag_count.most_common(15), \"\\n\")\n",
    "\n",
    "tag_bigrams_c = tag_bigrams\n",
    "tag_trigrams_c = tag_trigrams\n",
    "for excluded in exclude_list:\n",
    "    tag_bigrams_c = [tagbi for tagbi in tag_bigrams_c if excluded not in tagbi]\n",
    "    tag_trigrams_c = [tagtri for tagtri in tag_trigrams_c if excluded not in tagtri]\n",
    "tag_bigram_count = Counter(tag_bigrams_c)\n",
    "tag_trigram_count = Counter(tag_trigrams_c)\n",
    "print(tag_bigram_count.most_common(15), \"\\n\")\n",
    "print(tag_trigram_count.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'nationalsiblingsday' in ('fashion', 'nationalsiblingsday', 'tuesdaythoughts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "\n",
    "labels, counts = zip(*tag_count.most_common(15))\n",
    "\n",
    "pt = PrettyTable()\n",
    "pt.add_column(\"Single Hashtag\", labels)\n",
    "pt.add_column(\"Count\", counts)\n",
    "\n",
    "print(pt.get_html_string()) #print out html version of table, for ease of presentation on github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "\n",
    "labels, counts = zip(*tag_bigram_count.most_common(15))\n",
    "\n",
    "pt = PrettyTable()\n",
    "pt.add_column(\"Hashtag Bigrams\", labels)\n",
    "pt.add_column(\"Count\", counts)\n",
    "\n",
    "print(pt.get_html_string()) #print out html version of table, for ease of presentation on github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels, counts = zip(*tag_trigram_count.most_common(15))\n",
    "\n",
    "pt = PrettyTable()\n",
    "pt.add_column(\"Hashtag\", labels)\n",
    "pt.add_column(\"Count\", counts)\n",
    "\n",
    "print(pt.get_html_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[text for text in texts if '#equalpayday' in text.lower()][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[text for text in texts if '#zuckerburg' in text.lower()][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[text for text in texts if '#onlychild' in text.lower()][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#store the irrelevant hashtags\n",
    "irrel_hashtags = ['#equalpayday', '#tuesdaythoughts', '#lifecourbeeasier', '#zuckerberg', \\\n",
    "                  '#brochure', '#rack', '#flyer', '#roll',\\\n",
    "                 '#cbx_bloomingdays', '#felizmartes', '#temblor', '#mondaymotivation', \\\n",
    "                 '#americanidol', '#foodasitcom', '#michaelcohen', '#fcbsfc', '#fft18', '#michaelcohen',\\\n",
    "                 \"#onlychild\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyses of the tweet content - Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using the nltk package for analyses on the tweet content (i.e., text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!conda install -y -c anaconda nltk\n",
    "import nltk\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"stopwords\")\n",
    "from nltk.collocations import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import urllib\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "tokens = [tknzr.tokenize(text) for text in texts]\n",
    "tokens[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clean the token list\n",
    "clean_token_list = []\n",
    "stem_word_lookup = {}\n",
    "\n",
    "for tweet in tokens:\n",
    "    clean_tweet = []\n",
    "    for word in tweet:\n",
    "        if word.startswith(\"htt\") or word.startswith(\"@\") or word.startswith(\"#\") \\\n",
    "                            or word == \"RT\" \\\n",
    "                            or word in string.punctuation or word == \"â€™\" \\\n",
    "                            or word.lower() in stopwords.words('english'):   #this punctuation is not in the string punct list\n",
    "            continue # Skip this one!\n",
    "        else:\n",
    "            stem = ss.stem(word)\n",
    "            clean_tweet.append(stem)\n",
    "            if not stem in stem_word_lookup.keys():\n",
    "                stem_word_lookup[stem] = []\n",
    "            if not word in stem_word_lookup[stem]:\n",
    "                stem_word_lookup[stem].append(word)\n",
    "    clean_token_list.append(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_token_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combining all the tokens with sentinel indicating different tweets\n",
    "clean_token_join=[]\n",
    "for tokens in clean_token_list:\n",
    "    clean_token_join.append(' '.join(tokens))\n",
    "\n",
    "# Here's our sentinel word.\n",
    "sentinel = 'LLLLLLLLL'\n",
    "# For convenience, with spaces.\n",
    "spaced_sentinel = ' ' + sentinel + ' '\n",
    "combinedTokens=spaced_sentinel.join(clean_token_join).split()\n",
    "print(clean_token_join[0:10])\n",
    "combinedTokens[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigrams first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigrams = [n for n in ngrams(combinedTokens,1) if sentinel not in n]\n",
    "print(unigrams[0:20])\n",
    "unigram_frequency = Counter([\" \".join(gram) for gram in unigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram_frequency.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(unigram_frequency)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigrams = [n for n in ngrams(combinedTokens,2) if sentinel not in n]\n",
    "print(bigrams[0:20])\n",
    "bigram_frequency = Counter([\" \".join(gram) for gram in bigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_frequency.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(bigram_frequency)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigrams = [n for n in ngrams(combinedTokens,3) if sentinel not in n]\n",
    "print(trigrams[0:20])\n",
    "trigram_frequency = Counter([\" \".join(gram) for gram in trigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trigram_frequency.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(trigram_frequency)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyses of the tweet content - LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I perform LDA topic modeling to the clean token list to figure out common topics.\n",
    "I loop through 1 to 20 topics to obtain the model perplexity (lower is better) and the topic coherence (higher is better) of each topic model with certain number of topics predefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!conda install -y gensim \n",
    "from gensim import corpora\n",
    "vocab = corpora.Dictionary(clean_token_list)\n",
    "corpus = [vocab.doc2bow(tweet) for tweet in clean_token_list]\n",
    "corpus[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import ldamodel\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_lda = ldamodel.LdaModel(corpus, num_topics=5, id2word = vocab, passes = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_lda.log_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CoherenceModel(model=tweet_lda, texts=clean_token_list,\n",
    "                                        dictionary=vocab, coherence='c_v').get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perplexity = []\n",
    "coherence = []\n",
    "for i in range(1,21):\n",
    "    tweet_lda = ldamodel.LdaModel(corpus, num_topics=i, id2word = vocab, passes = 20)\n",
    "    perplexity.append(tweet_lda.log_perplexity(corpus))\n",
    "    coherence.append(CoherenceModel(model=tweet_lda, texts=clean_token_list,\n",
    "                                        dictionary=vocab, coherence='c_v').get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig1=plt.figure\n",
    "plt.plot(range(1,21,1), perplexity)\n",
    "plt.xticks(range(1,21,2))\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Model Perplexity')\n",
    "plt.savefig('LDA_perplexity.png')  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig2=plt.figure\n",
    "plt.plot(range(1,21), coherence)\n",
    "plt.xticks(range(1,21,2))\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Topic Coherence')\n",
    "plt.savefig('LDA_coherence.png')  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on output from perplexity and coherence, I decided to look into the 4-, 11-, and 20-topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4-topic model\n",
    "tweet_lda_4topic = ldamodel.LdaModel(corpus, num_topics=4, id2word = vocab, passes = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_lda_4topic.print_topics(num_topics=4, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic3_list=['research', 'keyword', 'profession', 'market', 'competitor', 'digit']\n",
    "topic3_text = [text for text in texts if any(word in text.lower() for word in topic3_list) \\\n",
    "               and not any(hashtag in text.lower() for hashtag in irrel_hashtags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(topic3_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random #for randomly picking example tweets\n",
    "[ topic3_text[i] for i in sorted(random.sample(range(len(topic3_text)), 20)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic3_list=['research', 'keyword', 'market','digit'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#11-topic model\n",
    "#tweet_lda_11topic = ldamodel.LdaModel(corpus, num_topics=11, id2word = vocab, passes = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tweet_lda_11topic.print_topics(num_topics=11, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyses of the tweet content - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before sentiment analyses, exclude all the irrelevant tweets that contained any of the irrelevant hashtags or keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irrel_list=topic3_list+irrel_hashtags\n",
    "texts_c = [text for text in texts if not any(word in text.lower() for word in irrel_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(texts_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compound = []\n",
    "neg = []\n",
    "pos = []\n",
    "neu = []\n",
    "for tweet in texts_c:\n",
    "    scores=sid.polarity_scores(tweet)\n",
    "    compound.append(scores.get(\"compound\"))\n",
    "    neg.append(scores.get(\"neg\"))\n",
    "    pos.append(scores.get(\"pos\"))\n",
    "    neu.append(scores.get(\"neu\"))\n",
    "    if len(compound)%5000==0:\n",
    "        print(len(compound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(compound) #make sure it's consistent with the texts list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(compound, 15, color=\"purple\", alpha=.5)\n",
    "plt.title(\"Compound Scores\")\n",
    "plt.savefig('sentiment_compound.png') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_zero=[]\n",
    "index_bitpos=[]\n",
    "index_verypos=[]\n",
    "index_neg=[]\n",
    "for i in range(len(compound)):\n",
    "    x=compound[i]\n",
    "    if x==0:\n",
    "        index_zero.append(i)\n",
    "    if x>0 and x<.50:\n",
    "        index_bitpos.append(i)\n",
    "    if x>.50:\n",
    "        index_verypos.append(i)\n",
    "    if x<0:\n",
    "        index_neg.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(index_zero))\n",
    "print(len(index_bitpos))\n",
    "print(len(index_verypos))\n",
    "print(len(index_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_zero=[texts[i] for i in index_zero]\n",
    "tweet_bitpos=[texts[i] for i in index_bitpos]\n",
    "tweet_verypos=[texts[i] for i in index_verypos]\n",
    "tweet_neg=[texts[i] for i in index_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[ tweet_zero[i] for i in sorted(random.sample(range(len(tweet_zero)), 20)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[ tweet_bitpos[i] for i in sorted(random.sample(range(len(tweet_bitpos)), 20)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[ tweet_verypos[i] for i in sorted(random.sample(range(len(tweet_verypos)), 20)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[ tweet_neg[i] for i in sorted(random.sample(range(len(tweet_neg)), 20)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(pos, 15, color=\"red\", alpha=.5)\n",
    "plt.title(\"Positive Scores\")\n",
    "plt.savefig('sentiment_pos.png') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_posdev=[]\n",
    "for i in range(len(pos)):\n",
    "    x=compound[i]\n",
    "    if x>.70:\n",
    "        index_posdev.append(i)\n",
    "print(len(index_posdev))\n",
    "tweet_posdev=[texts[i] for i in index_posdev]\n",
    "[tweet_posdev[i] for i in sorted(random.sample(range(len(tweet_posdev)), 20)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(neg, 15, color=\"blue\", alpha=.5)\n",
    "plt.savefig('sentiment_neg.png')\n",
    "plt.title(\"Negative Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_negdev=[]\n",
    "for i in range(len(neg)):\n",
    "    x=compound[i]\n",
    "    if x>.20:\n",
    "        index_negdev.append(i)\n",
    "print(len(index_negdev))\n",
    "tweet_negdev=[texts[i] for i in index_negdev]\n",
    "[tweet_negdev[i] for i in sorted(random.sample(range(len(tweet_negdev)), 20)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(neu, 15, color=\"black\", alpha=.5)\n",
    "plt.title(\"Neutral Scores\")\n",
    "plt.savefig('sentiment_neu.png') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_neudev=[]\n",
    "for i in range(len(neu)):\n",
    "    x=compound[i]\n",
    "    if x<.30:\n",
    "        index_neudev.append(i)\n",
    "print(len(index_neudev))\n",
    "tweet_neudev=[texts[i] for i in index_neudev]\n",
    "[tweet_neudev[i] for i in sorted(random.sample(range(len(tweet_neudev)), 20)) ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
